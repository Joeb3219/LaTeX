\documentclass[12]{article}
\usepackage[utf8]{inputenc}
\usepackage{blindtext}
\usepackage{amsmath}
\usepackage{fullpage}
\usepackage{setspace}
\usepackage [english]{babel}
\usepackage [autostyle, english = american]{csquotes}
\usepackage{indentfirst}
\MakeOuterQuote{"}

\usepackage[
backend=bibtex,
style=numeric,
sorting=none
]{biblatex}


\addbibresource{bibliography.bib}

\usepackage{etoolbox}
\AtBeginEnvironment{quote}{\singlespacing\small}

\title{Evaluating the Responsibility of the Programmer for His AI}
\author{Joseph A. Boyle}
\date{December 20, 2017}




\makeatletter
\pretocmd{\@sect}{\singlespacing}{}{}
\pretocmd{\@ssect}{\singlespacing}{}{}
\apptocmd{\@sect}{\doublespacing}{}{}
\apptocmd{\@ssect}{\doublespacing}{}{}
\makeatother



\begin{document}

\maketitle

\section{Introduction}
Since the days of the first computer systems, science fiction novels have spelled out the doom humanity will face once the "killer robots" wage war and take us out. As a society, we've often explored the bigger picture ideas such as what we might do if all of the computers suddenly whirred to life and stood arm in arm and shut down our country, but we seldom look at the smaller scale. If a single robot commits a crime, can we put it in jail or disassemble it? How large of a crime needs to be committed before it is acceptable to enact the "death penalty" for a robot? Or, should the penalties a robot accumulates be handed directly to the person who created it? If so, and the penalty of a crime is death, does its creator gain the death penalty? Throughout this paper, I will explore the various kinds of issues that could arise with robots, and when it is fair to blame the developer of the robot for those issues.

\section{Types of Robots}
	For the purposes of our discussion, the definition of a "bot" is important. Generally, the term \textit{bot} is used to refer to one of the three following types of Artificial Intelligence. The first type, \textit{non mobile bots}, are the types of AI that have been developed for the predominant duration of AI research. Non mobile bots include chatbots, infobots, weather-predicting bots, and news bots. Typically, non-mobile bots are viewed as computer programs rather than a conventional bot. Normally, bots of this type are thought to be harmless, but this class also includes AI which decide what ads to show you, how to delegate power to a city, and whether or not a person is innocent in a trial. The next type of bot,  \textit{mobile bots}, refer to the conventional notion of a "real life" bot that can move around, and generally look like Hanson Robotics's Sophia, Data from \textit{Star Trek}, or \textit{Wall-E}. Compared to non mobile bots, we tend to ascribe human characteristics to mobile bots, as they seem "life like". The final type of bot, \textit{reproductive bots}, refer to the class of robots which either can directly produce off-spring (either via "birth" or cloning) or can modify their own code or hardware without human assistance. 
	
	Generally, the complexity of developing a bot increases as we move up the chain -- that is, it's easier to develop a non-mobile bot than a mobile-bot than a reproductive bot -- but that's not always the case.	
	
\section{Assessing Faults for Harm Caused by Robots}
	It is useful to split our discussion into two pieces: robots that have intentionality, and robots that do not. Intentionality, generally, refers to the idea of an entity "[being] about \textit{something}"\cite{intentionality}. That is, an intentional agent has thoughts or beliefs about ideas or actions, rather than just being a mindless entity that does as is without second-guessing what their assignment is, where they are located, or who they are. To some, such as McCarthy and Dennett\cite{mccarthy,dennet}, intentionality seems to sometimes simply mean having some notion of an internal state of computation -- "a thermostat may usefully be ascribed one of exactly three beliefs—that the room is too cold, that it is too warm or that its temperature is ok"\cite{mccarthy}. Differently, we use intentionality to mean having beliefs about the act of doing a specific \textit{action}, having beliefs about what is going on beyond you in a sense other than that of processing (in effect, not just sensing what is going on around you to some result, but rather gathering information about your surroundings and having an internal thought process of how it makes you feel), and so on. The key feature of intentionality is that it carries with it \textit{feelings}, whereas systems that may have "beliefs" about their current state -- a thermostat believing that it is too hot or cold -- are merely variables in some program, and have no impact on the interior mental states of the entity. 
	
	 To be intentional in this sense necessarily implies consciousness, as having mental states and the ability to examine them implies consciousness, per McCarthy: "a robot will need many forms of self-consciousness, i.e. ability to observe its own mental state."\cite{mccarthy}  

	\subsection{Robots Without Intentionality}
		Imagine a \textit{non mobile bot} which decides what content to send its users. This bot, however, has been intentionally programmed to have a skew towards sending articles in favor of overthrowing the government. Over time, it influences public opinion and eventually causes havoc in society. This bot is doing the equivalent of a human wearing a blindfold and handing out whatever papers are at the top of the stack of papers. It has no thought process of what papers it hands out -- it just gives one to each passer-by in a rote-manner. If a malicious attacker were to replace all of the papers in the stack with pro-coup pamphlets, and the person handing out the papers continued following its program without any mental state, it's reasonable to conclude that the fault doesn't lie with the person handing out the papers. According to Searle\cite{chineseroom1, chineseroom2}:
	
		\begin{quote}
			Why does the man in the Chinese room not understand Chinese even though he can pass the Turing test for understanding Chinese? The answer is that he has only the formal syntax of the program and not the actual mental content or semantic content that is associated with the words of a language when a speaker understands that language.
		\end{quote}
	
		That is, if a robot is just a blind machine following a program as Searle says, we cannot reasonably attribute blame the robot, similar to the human situation. Can the same be said about robots which aren't as cut and dry as being developed by a programmer with malicious intent? Take, for instance, the AI that Facebook uses to serve targeted ads to its viewers. In the 2016 Presidential Election, this algorithm has been criticized as disproportionally showing Russian propaganda\cite{facebook_russia} in comparison with valid ads, therein potentially swaying the results of the election. Thus far, we have yet to find that a particular individual or even company has purposely caused this error, but we still seek to place the blame for the malfunction somewhere.
	
		We must truly consider the question of whether or not lacking intentionality is a justifiable reason to not be blamed for a malfunction. Previously, we asserted that an individual handing out destructive papers is okay if they have no way of knowing what they are doing, but I believe that requires a second contingency: that the entity also has no way to correct it. That is, it is only when an entity has no way of understanding what they are doing and also has no mechanism to deviate from that task, that they can be held liable. Given a robot that can't alter its own code or affect future generations of itself via its own free will, it seems clear that robots without intentionality don't fall into this group of agents who could receive blame for crimes.
	
		Without giving our mindless man handing out papers any intentionality, we give to him some added instructions: scanning each page and detecting if it contains any "Destruction" symbols (a process which is quite complicated, but for the sake of discussion we assume is simple). If a "Destruction" symbol appears on the paper, he is to not hand it out to anybody, whereas if it doesn't, he can hand the pamphlet out. There are two major ways in which this protocol could fail: the paper could have a negative symbol that we simply aren't searching for, or it could have one and we just miss it. Responsibility for mistakes in the system of the first type are very clearly not within the system's control, and therein must fall back to the developer. Responsibility for mistakes of the second type are much more difficult to assign, though. While the majority of the article exploring what it means to be conscious, Wertheim gives a pretty good account of what it means to be \textit{cognizant}\cite{zombie_within}:

		\begin{quote}
			He brings up an image of an airplane on a runway and tells me that when he presses a key some major feature will disappear. I am to tell him what it is. Koch jabs at the keyboard and the image flashes momentarily, but as far as I can tell everything remains the same. He does it again, several times, but still I see nothing different. Finally Koch tells me it is the aircraft’s fuselage that disappears. Once it’s pointed out, the omission becomes glaringly evident.
		\end{quote}
		
		For a computer system, being cognizant generally requires the system to know what it is looking at. There are numerous classifiers to detect birds, planes, buildings, and other general objects, but it is incredibly hard for a system to infer what type an object is without having ever been given the association before. This is equivalent to giving a human the task of identifying a species of bird from a picture when they had never before heard of a bird. Wertheim makes a strong point in his article that generally humans run on "zombie mode"\cite{zombie_within}, noticing just enough to get by in our day to day lives on "autopilot"\cite{zombie_within}. Until the fuselage was pointed out, its disappearance wasn't noticeable to most humans. In the meantime, it is reasonable to assume that the human test subject could see all of the parts of the plane and even identify them. Computer systems suffer from the opposite problem -- they can find the differences between two images or scenes, but don't intrinsically know what each of the parts are. Relating back to the problem at hand, a system which can't observe a symbol, for it wasn't shown what to look for, is not at fault, much in the way that we wouldn't blame a human for not noticing something that it had no reasonable way of detecting. If we must assign blame to somebody, then, it should go to whomever decided to employ such a bot given its limited ability -- most likely the programmer.

	\subsection{Robots With Intentionality}
		If a robot has intentionality and thus has beliefs about its environment, those beliefs are necessarily shaped by the sense of morality it has adopted. That is, to carry beliefs is to feel how you do because of some internal view of how the world should work, which can be referred to as morality. On a per-entity basis, morality isn't necessarily just -- a single entity may have fairly negative morals from the point of view of society as a whole. When given a task, a robot with a sense of morality -- whether good or bad -- ought to be able to decide whether or not to do that task based on if it fits within its morals. That is, if a robot has developed one moral and one moral only -- "I will not kill a human" -- and it is given the task of killing a human, it should be able to refuse that order or do something else to mitigate the situation. If it cannot do that, the system is more or less following a program mechanically, much as in the tasks we defined in the previous section, and thus the responsibility for the wrong-doing should be delegated to whomever designed the bot to not be able to resist unjust actions.
		
		If the bot is able to alter its path based on its moral compass, the question now becomes where it acquires those morals from. Allen et al. outlined three key ways in which an artificial agent may be taught morality: a "top-down approach" in which we specify a series of "rules" such as "The Ten Commandments" or "Asimov's Laws" that the agent must reside within, a "bottom-up approach" in which we "provide environments in which appropriate behavior is selected or rewarded", and a "hybric approach" which combines the previous two\cite{morality}. 
		
		First, let's consider the case in which the "top down approach"\cite{morality} is utilized. Since this approach essentially boils down to specifying the corner cases manually, it is inherently flawed for the same reasons as in the previous section: the set of rules we feed the agent are necessarily limited in size, depend on a thorough examination of all possible loopholes, and, most importantly, depend on the developer's sense of morality. If a developer were to instil a copy of their moral boundaries onto a robot, we carry the assumption that anything negative that the robot does is a reflection of the developer. That is, if the robot finds that it is okay to kill innocent civilians, the developer carries the burden of the responsibility because he chose to both create an AI that had the ability to decide what to do, but also to allow it to chose actions which have overlap with negative actions. To escape this responsibility, the developer could have either properly given a more restrictive rule-set to the bot (which eventually converges to the bot having no free will), or chosen not to give it free will.
		
		Next, let's consider the cases in which the "hybrid" or "bottom up" approaches\cite{morality} are utilized. Since the hybrid approach combines the previous two, it will have similar results to the parts it builds upon, depending upon how they're implemented. In the bottom-up approach, there must be either a set of actions which are beneficial and reward the bot, or a set of actions which are negative, and therein punish the bot. This feels like the most natural extension of "trial and error" or "evolution", in which the machine learns by doing to eventually reach the intellectual and moral level of man\cite{turing_test}. Thus, if a broken sense of morality is to develop, it ultimately falls upon the teacher who inspires what is and isn't good. In Turing's view, this could be a school teacher similar to those a child might see\cite{turing_test}, or perhaps it will be the programmer who designed the artificial intelligence. Regardless, the responsibility for any missteps in the bot's behavior, barring any technical errors (in effect, assuming perfect program execution), lies squarely with whomever teaches the morality to it via lessons.
		
		Finally, let's consider a small case of robots which can reproduce or modify their own code. This is one of the scariest possibilities, as it is one that we have the least control over -- if a robot decides it wants to modify its code and does so in a way that harms humanity, how do we assign blame? If taught via a bottom-up approach in which we show morality, then that blame should rightfully go to the teacher and developer of the robot -- afterall, without an immoral compass and without the ability to modify its own source code, such modifications wouldn't occur. Regardless, the blame also lies with the robot itself, and would likely lead to decommissioning if the variation is large enough to harm humanity. What about small variations passed through generations, though? If over one thousand generations, a breed of robots learns to strongly dislike humans, can we fairly say it's the original developer's fault any more? I'd say that the fault lies in the same place we put the blame for the awful things humanity has done to other species of animals -- within the system that gave us the evolutionary ability to exist. That is, as a species, it is our duty to ensure that robots being evolved from generation to generation don't deviate from one-another to ensure large deviations don't occur.

\section{Conclusion}
	One of the primary reasons that it is difficult to reason about who is to blame when something goes wrong in a philosophical sense, is due to the large implication that that decision carries. Inventing something that goes beyond the "hunk of metal" realm and into the "entity" realm carries with it a weight that is difficult to dismiss when things go wrong. Much in the way that our parents are responsible for many of our actions until we reach the legal age of the majority -- an age at which it is decided humans are "responsible", "conscious", and "moral" -- it is reasonable to assume that the creator of an AI has a number of responsibilities over it. What we have explored in this paper is a set of scenarios that could occur, each of which carries varying degrees of blame for the creator and creation, that we will likely have to deal with in the coming years.
	
	These answers are far from cut-and-dry, and oftentimes leave an unsatisfying answer: imagine a combat robot that is developed to only follow orders from a specific individual, resisting what it views as immoral orders. This works well and is able to only do the right thing. If that bot is now "hacked" into and made to ignore its moral compass and instead kill anybody in sight, can we assign blame to the original developer? On a surface glance, this answer seems trivial -- no, it's the hacker's fault. What if we find out that the developer introduced bugs into the source code of the robot which made is susceptible to hacking? Perhaps it's reasonable, then, to conclude that the developer is at fault. Consider, then, that the vulnerability is not from a bug that the developer introduced, but rather from the hacker finding a way to crack the underlying encryption technique that the developer used, which was created by yet another developer? It is in the nature of software development that behavior is abstracted away and built on top of. If we were to suddenly find an error in one of the lower layers and therein cause a robot built on top of those layers to exhibit errors, can we truly fault the developers of those bottom layers if they never knew that their work would be used in such a manor? This seems equivalent to the problem of a cell phone being used in a phone-activated, roadside bomb -- do we hold the cell phone manufacturer responsible? It is a slippery slope to assume a creator can foresee every possible usage of their invention. Rather, we must examine scenarios in which the developer either acted in malice or knew of fatal flaws and persevered anyway. 
	
	Throughout the scenarios explored, a few definitive trends have appeared. First, creations made with malicious intent are very clearly the responsibility of the creator, and very little the responsibility of the creation. Second, bots created without intentionality have much less responsibility for their actions compared with those that have intentionality, as they are mostly following. It seems, by and large, that it is quite easy to peg blame for a robot's actions on everybody else, which at first seems quite unnatural. Surely, if an AI is as intelligent as we claim it to be, then it has to have \textit{some} personal responsibility over its actions. Truth be told, though, the majority of system failures fall back into our hands -- misconfigured systems, untrained programs, untested scenarios, and unforeseen use-cases are all too common in the engineering world. It's very rare, especially with robots that arguably aren't yet conscious, to encounter a scenario in which a robot acts negatively against us in some way that humans couldn't have avoided by just going about it another way.
	
	In terms of responsibility for a programmer, then, that just leaves us with one answer: the programmer is responsible for as much as we're willing to put on them as a society. Perhaps if too much weight is assigned, we will scare away growth in the industry. Not assigning enough weight, though, is equivalent to letting a doctor get away with malpractice.

\newpage

\printbibliography[title={References}]

\end{document}