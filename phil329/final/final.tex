\documentclass[12]{article}
\usepackage[utf8]{inputenc}
\usepackage{blindtext}
\usepackage{amsmath}
\usepackage{fullpage}
\usepackage{setspace}
\usepackage [english]{babel}
\usepackage [autostyle, english = american]{csquotes}
\usepackage{indentfirst}
\MakeOuterQuote{"}

\usepackage[
backend=bibtex,
style=numeric,
sorting=none
]{biblatex}


\addbibresource{bibliography.bib}

\usepackage{etoolbox}
\AtBeginEnvironment{quote}{\singlespacing\small}

\title{Evaluating the Responsibility of the Programmer for His AI}
\author{Joseph A. Boyle}
\date{December 20, 2017}




\makeatletter
\pretocmd{\@sect}{\singlespacing}{}{}
\pretocmd{\@ssect}{\singlespacing}{}{}
\apptocmd{\@sect}{\doublespacing}{}{}
\apptocmd{\@ssect}{\doublespacing}{}{}
\makeatother



\begin{document}

\maketitle

\section{Introduction}
Since the days of Alan Turing\cite{turing_test}, the question of whether or not machines can truly think has troubled even the brightest computer scientists, philosophers, and cognitive scientists. Moreover, the consequences of that question carry heavy consequences which have made the discussion space difficult to prove one way or another. If a robot commits a war crime, does its ability to think and rationally decide to commit that crime leave it liable? If so, does the lack of a real thought process delegate responsibility elsewhere? If the penalty of a crime is death, and the robot isn't ultimately responsible, does its creator gain the death penalty?

\section{Types of Robots}
	For the purposes of our discussion, the definition of a "bot" is important. Generally, the term \textit{bot} is used to refer to one of the three following types of Artificial Intelligence. The first type, \textit{non mobile bots}, are the types of AI that have been developed for the predominant duration of AI research. Non mobile bots include chatbots, infobots, weather-predicting bots, and news bots. Typically, non-mobile bots are viewed as computer programs rather than a conventional bot. Normally, bots of this type are thought to be harmless, but this class also includes AI which decide what ads to show you, how to delegate power to a city, and whether or not a person is innocent in a trial. The next type of bot,  \textit{mobile bots} refer to the conventional notion of a "real life" bot that can move around, and generally look like Hanson Robotics's Sophia, Data from \textit{Star Trek}, or \textit{Wall-E}. Compared to non mobile bots, we tend to ascribe human characteristics to mobile bots, as they seem "life like". The final type of bot, \textit{reproductive bots}, refer to the class of robots which either can directly produce off-spring (either via "birth" or cloning) or can modify their own code or hardware without human assistance. 
	
	Generally, the complexity of developing a bot increases as we move up the chain -- that is, it's easier to develop a non-mobile bot than a mobile-bot than a reproductive bot -- but that's not always the case.	
	
\section{Assessing Faults for Harm Caused by Robots}
	It is useful to split our discussion into two pieces: robots that have intentionality, and robots that do not. Intentionality, generally, refers to the idea of an entity "[being] about \textit{something}"\cite{intentionality}. That is, an intentional agent has thoughts or beliefs about ideas or actions, rather than just being a mindless entity that does as is without second-guessing what their assignment is, where they are located, or who they are. To some, such as McCarthy and Dennett\cite{mccarthy,dennet}, intentionality seems to sometimes simply mean having some notion of an internal state of computation -- "a thermostat may usefully be ascribed one of exactly three beliefs—that the room is too cold, that it is too warm or that its temperature is ok"\cite{mccarthy}. Differently, we use intentionality to mean having beliefs about the act of doing a specific \textit{action}, having beliefs about what is going on beyond you in a sense other than that of processing (in effect, not just sensing what is going on around you to some result, but rather gathering information about your surroundings and having an internal thought process of how it makes you feel), and so on. The key feature of intentionality is that it carries with it \textit{feelings}, whereas systems that may have "beliefs" about their current state -- a thermostat believing that it is too hot or cold -- are merely variables in some program, and have no impact on the interior mental states of the entity. 
	
	 To be intentional in this sense necessarily implies consciousness, as having mental states and the ability to examine them implies consciousness, per McCarthy: "a robot will need many forms of self-consciousness, i.e. ability to observe its own mental state."\cite{mccarthy}  

	\subsection{Robots Without Intentionality}
		Imagine a \textit{non mobile bot} which decides what content to send its users. This bot, however, has been intentionally programmed to have a skew towards sending articles in favor of overthrowing the government. Over time, it influences public opinion and eventually causes havoc in society. Perhaps another bot gives stock advice to investors, and was programmed to intentionally give advice that will bankrupt the investor in favor of bringing wealth to the developer. According to Searle\cite{chineseroom1, chineseroom2},
	
		\begin{quote}
			Why does the man in the Chinese room not understand Chinese even though he can pass the Turing test for understanding Chinese? The answer is that he has only the formal syntax of the program and not the actual mental content or semantic content that is associated with the words of a language when a speaker understands that language.
		\end{quote}
	
		Thus, the bot is doing the equivalent of a human wearing a blindfold and handing out whatever papers are at the top of the stack of papers. It has no thought process of what papers it hands out -- it just gives one to each passer-by in a rote-manner. If a malicious attacker were to replace all of the papers in the stack with pro-coup pamphlets, and the person handing out the papers continued following its program without any mental state, it's reasonable to conclude that the fault doesn't lie with the person handing out the papers. Similarly, if a robot is just a blind machine following a program as Searle says, we cannot reasonably attribute blame the robot. 
	
		Can the same be said about robots which aren't as cut and dry as being developed by a programmer with malicious intent? Take, for instance, the AI that Facebook uses to serve targeted ads to its viewers. In the 2016 Presidential Election, this algorithm has been criticized as disproportionally showing Russian propaganda\cite{facebook_russia} in comparison with valid ads, therein potentially swaying the results of the election. Thus far, we have yet to find that a particular individual or even company has purposely caused this error, but we still seek to place the blame for the malfunction somewhere.
	
		We must truly consider the question of whether or not lacking intentionality is a justifiable reason to not be blamed for a malfunction. Previously, we asserted that an individual handing out destructive papers is okay if they have no way of knowing what they are doing, but I believe that requires a second contingency: that the entity also has no way to correct it. That is, it is only when an entity has no way of understanding what they are doing and also has no mechanism to deviate from that task, that they can be held liable. Given a robot that can't alter its own code or affect future generations of itself via its own free will, it seems clear that robots without intentionality don't fall into this group of agents who could receive blame for crimes.
	
		Without giving our mindless man handing out papers any intentionality, we give to him some added instructions: scanning each page and detecting if it contains any "Destruction" symbols (a process which is quite complicated, but for the sake of discussion we assume is simple). If a "Destruction" symbol appears on the paper, he is to not hand it out to anybody, whereas if it doesn't, he can hand the pamphlet out. There are two major ways in which this protocol could fail: the paper could have a negative symbol that we simply aren't searching for, or it could have one and we just miss it. Responsibility for mistakes in the system of the first type are very clearly not within the system's control, and therein must fall back to the developer. Responsibility for mistakes of the second type are much more difficult to assign, though. While the majority of the article exploring what it means to be conscious, Wertheim gives a pretty good account of what it means to be \textit{cognizant}\cite{zombie_within}:

		\begin{quote}
			He brings up an image of an airplane on a runway and tells me that when he presses a key some major feature will disappear. I am to tell him what it is. Koch jabs at the keyboard and the image flashes momentarily, but as far as I can tell everything remains the same. He does it again, several times, but still I see nothing different. Finally Koch tells me it is the aircraft’s fuselage that disappears. Once it’s pointed out, the omission becomes glaringly evident.
		\end{quote}
		
		For a computer system, being cognizant generally requires the system to know what it is looking at. There are numerous classifiers to detect birds, planes, buildings, and other general objects, but it is incredibly hard for a system to infer what type an object is without having ever been given the association before. This is equivalent to giving a human the task of identifying a species of bird from a picture when they had never before heard of a bird. Wertheim makes a strong point in his article that generally humans run on "zombie mode"\cite{zombie_within}, noticing just enough to get by in our day to day lives on "autopilot"\cite{zombie_within}. Until the fuselage was pointed out, its disappearance wasn't noticeable to most humans. In the meantime, it is reasonable to assume that the human test subject could see all of the parts of the plane and even identify them. Computer systems suffer from the opposite problem -- they can find the differences between two images or scenes, but don't intrinsically know what each of the parts are. Relating back to the problem at hand, a system which can't observe a symbol, for it wasn't shown what to look for, is not its fault, much in the way that we wouldn't blame a human for not noticing something that it had no reasonable way of detecting.

	\subsection{Robots With Intentionality}
		If a robot has intentionality and thus has beliefs about its environment, those beliefs are necessarily shaped by the sense of morality it has adopted. That is, to carry beliefs is to feel how you do because of some internal view of how the world should work, which can be referred to as morality. On a per-entity basis, morality isn't necessarily just -- a single entity may have fairly negative morals from the point of view of society as a whole. When given a task, a robot with a sense of morality -- whether good or bad -- ought to be able to decide whether or not to do that task based on if it fits within its morals. That is, if a robot has developed one moral and one moral only -- "I will not kill a human" -- and it is given the task of killing a human, it should be able to refuse that order or do something else to mitigate the situation. If it cannot do that, the system is more or less following a program mechanically, much as in the tasks we defined in the previous section, and thus the responsibility for the wrong-doing should be delegated to whomever designed the bot to not be able to resist unjust actions.
		
		If the bot is able to alter its path based on its moral compass, the question now becomes where it acquires those morals from. Allen et al. outlined three key ways in which an artificial agent may be taught morality: a "top-down approach" in which we specify a series of "rules" such as "The Ten Commandments" or "Asimov's Laws" that the agent must reside within, a "bottom-up approach" in which we "provide environments in which appropriate behavior is selected or rewarded", and a "hybric approach" which combines the previous two\cite{morality}. 
		
		First, let's consider the case in which the "top down approach"\cite{morality} is utilized. Since this approach essentially boils down to specifying the corner cases manually, it is inherently flawed for the same reasons as in the previous section: the set of rules we feed the agent are necessarily limited in size, depend on a thorough examination of all possible loopholes, and, most importantly, depend on the developer's sense of morality. If a developer were to instil a copy of their moral boundaries onto a robot, we carry the assumption that anything negative that the robot does is a reflection of the developer.  

\section{Conclusion}
	One of the primary reasons that it is difficult to reason about who is to blame when something goes wrong in a philosophical sense, is due to the large implication that that decision carries. Inventing something that goes beyond the "hunk of metal" realm and into the "entity" realm carries with it a weight that is difficult to dismiss when things go wrong. Much in the way that our parents are responsible for many of our actions until we reach the legal age of the majority -- an age at which it is decided humans are "responsible", "conscious", and "moral" -- it is reasonable to assume that the creator of an AI has a number of responsibilities over it. What we have explored in this paper is a set of scenarios that could occur, each of which carries varying degrees of blame for the creator and creation, that we will likely have to deal with in the coming years.
	
	Imagine a combat robot that is developed to only follow orders from a specific individual, resisting what it views as immoral orders. This works well and is able to only do the right thing. If that bot is now "hacked" into and made to ignore its moral compass and instead kill anybody in sight, can we assign blame to the original developer? On a surface glance, this answer seems trivial -- no, it's the hacker's fault. What if we find out that the developer introduced bugs into the source code of the robot which made is susceptible to hacking? Perhaps it's reasonable, then, to conclude that the developer is at fault. Consider, then, that the vulnerability is not from a bug that the developer introduced, but rather from the hacker finding a way to crack the underlying encryption technique that the developer used, which was created by yet another developer? It is in the nature of software development that behavior is abstracted away and built on top of. If we were to suddenly find an error in one of the lower layers and therein cause a robot built on top of those layers to exhibit errors, can we truly fault the developers of those bottom layers if they never knew that their work would be used in such a manor? This seems equivalent to the problem of a cell phone being used in a phone-activated, roadside bomb -- do we hold the cell phone manufacturer responsible? It is a slippery slope to assume a creator can foresee every possible usage of their invention. Rather, we must examine scenarios in which the developer either acted in malice or knew of fatal flaws and persevered anyway. 	
	
	Throughout the scenarios explored, a few definitive trends have appeared. First, creations made with malicious intent are very clearly the responsibility of the creator, and very little the responsibility of the creation. Second, bots created without intentionality have much less responsibility for their actions compared with those that have intentionality, as they are mostly following 

\newpage

\printbibliography[title={References}]

\end{document}